### Federated Learning 论文

#### The Non-IID Data Quagmire of Decentralized Machine Learning

##### **Some jargon**：

* **data skew**：数据倾斜，数据分布不均匀。
* **skewed distribution**：偏斜分布，可以联想正态分布。
* **IID**：独立同分布。在概率统计理论中，指随机过程中，任何时刻的取值都为随机变量，如果这些随机变量服从同一分布，并且互相独立，那么这些随机变量是独立同分布。



##### **Challenges**：

* Training a model over decentralized data using traditional training approaches requires massive amounts of communication. Doing so drastically slows down the training process because the communication is bottlenecked by the limited wide-area or mobile network bandwidth.

  **分布式学习的communication具体是什么？**为什么会受到网络的瓶颈？

  `补充：这里的communication应该指的是不同的训练节点在epoch训练完后与其他节点的通讯，从而更新参数、梯度`

* decentralized data is typically generated at different contexts, which can lead to significant differences in the distribution of data across data partitions.



##### **Solutions**：

* Group normalization 群归一化。avoids the skew-induced accuracy loss of batch normalization under BSP。（`补充：BSP同步训练,还有ASP等，分布式学习中的jargon`）

  在群归一化的条件下，这个问题可以被简化为准确性和沟通量的权衡？（要准确性则沟通量大）

* **SkewScout做的事情**：periodically sends local models to remote data partitions and compares the model performance (e.g., validation accuracy) between local and remote partitions.			 Based on the accuracy loss, SkewScout adjusts the amount of communication among data partitions by controlling how relaxed the decentralized learning algorithms should be, such as controlling the threshold that determines which parameters are worthy of communication



* 证明数据迁移对分布式学习的影响是基本和普遍的
* 构建了数据集
* 对于DNN，使用group normalization而非batch normalization
* 问题的难度取决于数据倾斜的程度
* 使用SkewScout 调整数据分区之间的通信频率，以反映数据的偏度，在保持模型准确性的同时，寻求最大限度地节省通信。



##### **三种分布式学习算法：**

* **Gaia**，每个partition自己更新自己的模型参数w，减少不必要的通信除非相对大小超过指定的阈值。

  超参数$T_0$，在分布式学习中决定是否更新$w_j$的最开始的阈值。

* **FederatedAveraging**，算法在每个 epoch 中选择所有分区的一个子集，每个选择的在本地运行预先指定数量次的 SGD，并将生成的模型传回中央服务器。服务器对所有这些模型取平均值，并使用平均后的模型参数来进行下一个 epoch 的训练。

  超参数$Iter_{Local}$，控制每一个par。tition的本地SGD的步数。

* **DeepGradient Compression**：通过只发送那些重要的梯度值来减少通信带宽，具体的说，只有大于阈值的梯度值会被发送出去，而那些小于阈值的梯度会在本地积累，直到大于阈值后再被发送。此外，该算法还引入了 (momentum correction, gradient clipping，momentum factor masking, warm-up training) 等方法。

  超参数$s$

BSP是有助于最后模型的准确性的，但由于需要communication太多，效率上不是很好。

Gaia和FederatedAveraging对于非IID数据的效果不是很好（50％的accuracy对二元问题来说比随便猜还差？）



##### Experiment

* **实验一**：证明数据偏移会令模型的质量下降；即使是每一轮训练结束后都进行一次通信的BSP也不能完全解决这个问题。
* **实验二**：BatchNorm 依赖于 mini-batch 的均值和方差，并且假设 minibatch 的均值与方差等于全局的均值与方差。然而在Non-IID的情况下，minibatch与全局的均值方差差距较大，这样导致 Non-IID 情况下不能简单地增加 batch 大小或使用更好的 batch 抽样来解决这个问题，因为此时每个数据分区 中的数据集并不能表示全局数据集。
* **实验三**：数据偏移的程度越高，对模型的影响越大。



##### 分布式的学习

在论文复现之前，需要先理解GeePS，也就是具体是如何实现分布式学习的。（源代码是建立在GeePS框架上）这里就暂时不看GeePS的论文了…通过[官网](https://blog.acolyer.org/2016/04/27/geeps-scalable-deep-learning-on-distributed-gpus-with-a-gpu-specialized-parameter-server/)和[别人的介绍](http://www.360doc.com/content/17/0211/09/37113458_628196914.shtml)迅速了解一下。

再在此之前，还要再了解一下分布式机器学习的相关内容。https://zhuanlan.zhihu.com/p/29032307

* 数据并行一般应用于数据很大模型很小，扫一个epoch需要很多时间时；
* 模型并行则应用于模型太大，一台机器或GPU放不下时（如大规模的Linear Regression，有一堆的参数）

无论是数据还是模型并行，最终都需要做一些“同步”任务，以保证最终结果能收敛。理想情况是线性加速（多一个计算节点速度加一），但一般同步工作会产生额外开销，甚至可能导致比单机还慢。

一个用来同步不同节点上计算任务的方法也就是参数服务器（PS）。

**参数服务器（PS）**：用来同步不同节点上计算任务的方法。



一般情况下的分布式机器学习流程：

$for\  \ t =1 \to T:\theta^{(t+1)} = \theta^{(t)} +\epsilon \sum_{p=1}^P \Delta \iota(\theta^{(t)},D_p^{(t)})$

这里我们有P个计算节点在不同的数据上同时执行计算函数$\Delta \iota$。当所有节点的计算都执行完毕后，它们的结果被汇总之后（ ∑ 符号）再用来更新参数$\theta$。

当在多机集群上进行训练时，不管是在计算开始前读取模型参数，还是在计算结束后收集多个节点上的梯度，都会涉及到网络通讯，如何保证参数共享和梯度同步即为要解决的核心问题。

这里就引出参数服务器了。简单地说，PS的服务器端维护全局共享的模型参数$ \theta^{(t)}$（我们通常称为parameter state），而客户端则对应到执行计算任务 Δ 的各个工作节点；同时，服务器端向客户端提供两个主要的API： push和pull。

* 在每个iteration开始的时候，所有的客户端先调用pull API向服务器发送一个请求，请求服务器回传最新的模型参数$ \theta^{(t)}$。当每个计算节点收到$ \theta^{(t)}$后，它就把这份最新的参数拷贝并覆盖到之前旧的参数$ \theta^{(t-1)}$上（物理上通常这些参数存储在RAM或GPU内存上），然后执行Δ 函数计算得到梯度更新值。换句话说，PS的pull API确保了每个计算节点在计算开始前都能获取一份最新参数的拷贝。
* 另一方面，梯度更新值计算完毕后，每个计算节点随后调用push API，把这组更新值发给服务器。服务器会收集所有计算节点发来的更新值，并且将它们“加”到当前维护的全局共享参数上$ \theta^{(t)} \to  \theta^{(t+1)}$。在下一个iteration，当服务器再次收到客户端的pull请求时，它就会把更新过后的 $ \theta^{(t+1)}$发出去。因此，push API确保了梯度值的收集和模型参数的更新。



**从CPU到GPU**

用PS把一个在单机上运行的神经网络训练程序并行化到 P 个机器上:

$for \  t = 1 \to T:$

$\  \  \ pull(\theta^{(t)})$

$\  \  \ \nabla \theta^{(t)}=\Delta \iota(\theta^{(t)},D_P^{(t)})$

$\  \  \ push(\nabla \theta^{(t)})$

如果把核心计算函数$\Delta \iota$挪到GPU上执行，由于GPU有自己的内存，所以所需要的输入和输出也要位于GPU内存上，在分布式环境下，$\theta^{(t)}$作为$\Delta \iota$的一个输入，是从远端pull来的，所以在之后还需要有一个memcpy操作，把参数移动到GPU内存上；同样，算完后也要从GPU移到RAM上再push。

这里就要引出分布式学习所面临的问题了：

* **内存移动**：在GPU和RAM之间拷贝数据会产生一定开销。完成一步计算和调用一个GPU内核程序花的时间是差不多的。
* **通讯**：每个iteration，服务器端必须接收到所有计算节点的梯度更新值，更新参数后才响应下一轮pull请求。由于各机器快慢不一，网络宽带也不一定充裕，需要花相当长的时间纯粹用来等各个节点的通讯完成。
* **GPU内存**

关于通讯问题，PS里面的“服务器”和“客户端”是一个虚拟的概念，在实际工程中，可以让集群中的任何一个物理节点既充当计算节点（worker）又是一个服务器（server）。这样我们就可以把服务器端实现成一个分布式的存储系统 -- 我们要服务器要维护的模型参数平均分布到多台物理机器，从而可以充分利用每台机器的通讯带宽了。

